{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 專案名稱: 第四屆機器學習百日馬拉松\n",
    "### 功能描述: 第39天作業\n",
    "### 版權所有: Dunk  \n",
    "### 程式撰寫: Dunk  \n",
    "### 撰寫日期：2020/04/10\n",
    "### 改版日期:  \n",
    "### 改版備註:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "清楚了解 L1, L2 的意義與差異為何，並了解 LASSO 與 Ridge 之間的差異與使用情境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請閱讀相關文獻，並回答下列問題\n",
    "\n",
    "[脊回歸 (Ridge Regression)](https://blog.csdn.net/daunxx/article/details/51578787)\n",
    "[Linear, Ridge, Lasso Regression 本質區別](https://www.zhihu.com/question/38121173)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection\n",
    "2. 當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?\n",
    ">因此進行迴歸分析時，當遭遇以下幾種情況時，可能要注意迴歸模式中是否存在多元共線性（multi-collinearity）的問題：\n",
    ">>1. 共線性指標超過標準（含容忍度tolerance、變異膨脹因子VIF、條件指標CI）\n",
    ">>2. 迴歸係數的方向性與相關係數相反\n",
    ">>3. 解釋力R平方過高，但個別變項的係數未達顯著水準。\n",
    ">\n",
    ">解決多元共線性的統計模式或方式還蠻多的，可以優先考慮的第一種作法是將選擇變數的方法改為逐步法（stepwise），若能將存在共線性的自變數排除在模式之外，可大幅改善統計結果的不合理；第二種作法，則是先利用主成分分析（principal components analysis，PCA）將線性重合的自變數重新建構成新的潛在變項（主成分得分），來替代原有的自變數，詳細的操作流程可參考（吳明隆，2009。SPSS操作與應用–問卷統計分析實務。臺北市：五南），不過主成分分數的命名，又是另一個頭痛的問題；第三種作法，則是本篇下方即將介紹的脊迴歸（ridge regression）。\n",
    ">\n",
    ">脊迴歸是一種修改最小平方法，允許有偏估計量，進而改善多元共線性的方法，一個簡單易懂的圖示如下（假設真實值為β，不偏估計式所求得統計量為b，有偏估計式所求得統計量為bR），估計量b雖然不偏，但因為標準誤較大，因此估計結果較不精確，此時我們會傾向選擇有偏估計量bR，雖然有偏誤，但bR落在真實值β的機率會高於不偏估計量b。\n",
    ">\n",
    ">此脊迴歸方程式有以下幾點特性：\n",
    ">>1. 當偏化常數c=0時，即為原先的不偏估計式；\n",
    ">>2. 當c值由0開始微些增加時，此時估計參數bR的改變幅度最大，甚至發生係數正負值的改變，隨著c值再增加時，迴歸係數bR的改變幅度會不斷變小，且迴歸係數bR越趨近於0；\n",
    ">>3. 個別VIF和估計參數bR的情形一樣，當c值由0開始些微增加時，VIF值會迅速下降，隨著c值再增加時，VIF的下降幅度會不斷減少；（4）當偏化常數c不斷增加時，迴歸模式的解釋力R2會不斷降低。\n",
    ">\n",
    ">根據以上幾點特性，在進行脊迴歸分析時，會建議從脊跡選擇一個迴歸係數趨於穩定（變項迴歸係數的正負值合理），VIF夠小（盡量維持在1~2之間），且c值盡量越小越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 參考資料\n",
    "[讀者提問：多元迴歸分析的變數選擇](https://taweihuang.hpd.io/2016/09/12/%E8%AE%80%E8%80%85%E6%8F%90%E5%95%8F%EF%BC%9A%E5%A4%9A%E5%85%83%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90%E7%9A%84%E8%AE%8A%E6%95%B8%E9%81%B8%E6%93%87/ \"讀者提問：多元迴歸分析的變數選擇\")\n",
    "\n",
    "[Lasso算法](https://zh.wikipedia.org/wiki/Lasso%E7%AE%97%E6%B3%95 \"Lasso算法\")\n",
    "\n",
    "[从Lasso开始说起- 知乎](https://zhuanlan.zhihu.com/p/46999826 \"从Lasso开始说起- 知乎\")\n",
    "\n",
    "[利用NCSS的脊迴歸（ridge regression）解釋多元共線性（multi-collinearity）~(1)](https://dasanlin888.pixnet.net/blog/post/442485077-%E5%88%A9%E7%94%A8ncss%E7%9A%84%E8%84%8A%E8%BF%B4%E6%AD%B8%EF%BC%88ridge-regression%EF%BC%89%E8%A7%A3%E9%87%8B%E5%A4%9A%E5%85%83%E5%85%B1 \"利用NCSS的脊迴歸（ridge regression）解釋多元共線性（multi-collinearity）~(1)\")\n",
    "\n",
    "[七種迴歸分析方法 個個經典](https://codertw.com/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80/45092/ \"七種迴歸分析方法 個個經典\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
