{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 專案名稱: 第四屆機器學習百日馬拉松\n",
    "### 功能描述: 第101~103天作業\n",
    "### 版權所有: \n",
    "### 程式撰寫: Dunk  \n",
    "### 撰寫日期：2020/07/15\n",
    "### 改版日期:  \n",
    "### 改版備註:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 套用第三屆的寫法了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH  = './image_data/train'\n",
    "TEST_PATH = './image_data/test'\n",
    "OUTPUT_PATH = './working'\n",
    "NUM_CLASSES = 5\n",
    "NUM_EPOCHS = 32\n",
    "SEED = 77\n",
    "# saved model\n",
    "WEIGHTS_FINAL = 'model-InceptionResNetV2.h5'\n",
    "categories=os.listdir(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2260 images belonging to 5 classes.\n",
      "Found 563 images belonging to 5 classes.\n",
      "Class #0 = daisy\n",
      "Class #1 = dandelion\n",
      "Class #2 = rose\n",
      "Class #3 = sunflower\n",
      "Class #4 = tulip\n"
     ]
    }
   ],
   "source": [
    "# Image preprocess\n",
    "train_datagen = ImageDataGenerator(rotation_range=30, width_shift_range=0.125, height_shift_range=0.125, zoom_range=0.125, horizontal_flip=True,\n",
    "                                   validation_split=0.2, rescale=1. / 255)\n",
    "train_batches = train_datagen.flow_from_directory(DATASET_PATH, subset = 'training', seed = SEED)\n",
    "valid_batches = train_datagen.flow_from_directory(DATASET_PATH, subset = 'validation', seed = SEED)\n",
    "for cls, idx in train_batches.class_indices.items():\n",
    "    print('Class #{} = {}'.format(idx, cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "net = InceptionResNetV2(include_top=False, input_shape=(256, 256, 3))\n",
    "x = net.output\n",
    "x = Flatten()(x)\n",
    "# Add Dense layer, each probability by softmax\n",
    "output_layer = Dense(NUM_CLASSES, activation='softmax', name='softmax')(x)\n",
    "net_final = Model(inputs=net.input, outputs=output_layer)\n",
    "net_final.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "71/71 [==============================] - 3053s 43s/step - loss: 1.6593 - accuracy: 0.6898 - val_loss: 17696.2031 - val_accuracy: 0.1812\n",
      "Epoch 2/32\n",
      "71/71 [==============================] - 3208s 45s/step - loss: 1.3479 - accuracy: 0.7137 - val_loss: 191481.5000 - val_accuracy: 0.4671\n",
      "Epoch 3/32\n",
      "71/71 [==============================] - 3070s 43s/step - loss: 1.0378 - accuracy: 0.7673 - val_loss: 1.0209 - val_accuracy: 0.6430\n",
      "Epoch 4/32\n",
      "71/71 [==============================] - 3031s 43s/step - loss: 0.6994 - accuracy: 0.8164 - val_loss: 481.0917 - val_accuracy: 0.2540\n",
      "Epoch 5/32\n",
      "71/71 [==============================] - 3021s 43s/step - loss: 0.6419 - accuracy: 0.7951 - val_loss: 0.2421 - val_accuracy: 0.7833\n",
      "Epoch 6/32\n",
      "71/71 [==============================] - 3002s 42s/step - loss: 2.5765 - accuracy: 0.7363 - val_loss: 405615.9688 - val_accuracy: 0.1705\n",
      "Epoch 7/32\n",
      "71/71 [==============================] - 3049s 43s/step - loss: 2.0387 - accuracy: 0.5956 - val_loss: 27893.2168 - val_accuracy: 0.2398\n",
      "Epoch 8/32\n",
      "71/71 [==============================] - 3056s 43s/step - loss: 1.0318 - accuracy: 0.6881 - val_loss: 31.5045 - val_accuracy: 0.5400\n",
      "Epoch 9/32\n",
      "71/71 [==============================] - 3087s 43s/step - loss: 0.8448 - accuracy: 0.7394 - val_loss: 1.1140 - val_accuracy: 0.6607\n",
      "Epoch 10/32\n",
      "71/71 [==============================] - 3112s 44s/step - loss: 0.7442 - accuracy: 0.7580 - val_loss: 0.4564 - val_accuracy: 0.7353\n",
      "Epoch 11/32\n",
      "71/71 [==============================] - 3087s 43s/step - loss: 0.6989 - accuracy: 0.7721 - val_loss: 0.9234 - val_accuracy: 0.7584\n",
      "Epoch 12/32\n",
      "71/71 [==============================] - 2932s 41s/step - loss: 0.7449 - accuracy: 0.7801 - val_loss: 2.7575 - val_accuracy: 0.4121\n",
      "Epoch 13/32\n",
      "71/71 [==============================] - 2970s 42s/step - loss: 0.9912 - accuracy: 0.7150 - val_loss: 55.6480 - val_accuracy: 0.5915\n",
      "Epoch 14/32\n",
      "71/71 [==============================] - 3041s 43s/step - loss: 0.6755 - accuracy: 0.7562 - val_loss: 0.7350 - val_accuracy: 0.7425\n",
      "Epoch 15/32\n",
      "71/71 [==============================] - 3070s 43s/step - loss: 0.6244 - accuracy: 0.7987 - val_loss: 368.9584 - val_accuracy: 0.6128\n",
      "Epoch 16/32\n",
      "71/71 [==============================] - 3074s 43s/step - loss: 0.6220 - accuracy: 0.8009 - val_loss: 0.4730 - val_accuracy: 0.7869\n",
      "Epoch 17/32\n",
      "71/71 [==============================] - 3069s 43s/step - loss: 0.5956 - accuracy: 0.8084 - val_loss: 0.5794 - val_accuracy: 0.7140\n",
      "Epoch 18/32\n",
      "71/71 [==============================] - 3073s 43s/step - loss: 0.5565 - accuracy: 0.8221 - val_loss: 0.4253 - val_accuracy: 0.7584\n",
      "Epoch 19/32\n",
      "71/71 [==============================] - 3071s 43s/step - loss: 0.5407 - accuracy: 0.8199 - val_loss: 0.3311 - val_accuracy: 0.7922\n",
      "Epoch 20/32\n",
      "71/71 [==============================] - 3098s 44s/step - loss: 0.4984 - accuracy: 0.8372 - val_loss: 0.2088 - val_accuracy: 0.8188\n",
      "Epoch 21/32\n",
      "71/71 [==============================] - 3099s 44s/step - loss: 0.4285 - accuracy: 0.8513 - val_loss: 0.6754 - val_accuracy: 0.7993\n",
      "Epoch 22/32\n",
      "71/71 [==============================] - 3099s 44s/step - loss: 0.7166 - accuracy: 0.8146 - val_loss: 0.4941 - val_accuracy: 0.7798\n",
      "Epoch 23/32\n",
      "71/71 [==============================] - 3080s 43s/step - loss: 0.4811 - accuracy: 0.8305 - val_loss: 0.2527 - val_accuracy: 0.8437\n",
      "Epoch 24/32\n",
      "71/71 [==============================] - 3084s 43s/step - loss: 0.4484 - accuracy: 0.8451 - val_loss: 0.7464 - val_accuracy: 0.7513\n",
      "Epoch 25/32\n",
      "71/71 [==============================] - 3029s 43s/step - loss: 0.4256 - accuracy: 0.8668 - val_loss: 20.6494 - val_accuracy: 0.7496\n",
      "Epoch 26/32\n",
      "71/71 [==============================] - 2961s 42s/step - loss: 0.3995 - accuracy: 0.8757 - val_loss: 0.4792 - val_accuracy: 0.8171\n",
      "Epoch 27/32\n",
      "71/71 [==============================] - 2852s 40s/step - loss: 0.3455 - accuracy: 0.8885 - val_loss: 0.1296 - val_accuracy: 0.8242\n",
      "Epoch 28/32\n",
      "71/71 [==============================] - 2819s 40s/step - loss: 0.3553 - accuracy: 0.8827 - val_loss: 0.5784 - val_accuracy: 0.8188\n",
      "Epoch 29/32\n",
      "71/71 [==============================] - 2854s 40s/step - loss: 0.3966 - accuracy: 0.8801 - val_loss: 0.0827 - val_accuracy: 0.7815\n",
      "Epoch 30/32\n",
      "71/71 [==============================] - 2970s 42s/step - loss: 0.3539 - accuracy: 0.8850 - val_loss: 0.3499 - val_accuracy: 0.8437\n",
      "Epoch 31/32\n",
      "71/71 [==============================] - 3003s 42s/step - loss: 0.3890 - accuracy: 0.8951 - val_loss: 0.4115 - val_accuracy: 0.7762\n",
      "Epoch 32/32\n",
      "71/71 [==============================] - 3008s 42s/step - loss: 0.4210 - accuracy: 0.8832 - val_loss: 0.7439 - val_accuracy: 0.7460\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "net_final.fit_generator(train_batches, validation_data = valid_batches, epochs = NUM_EPOCHS)\n",
    "# Store Model\n",
    "net_final.save(WEIGHTS_FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "out = np.array(['id', 'flower_class'])\n",
    "testfiles=os.listdir(TEST_PATH)\n",
    "for testfile in testfiles:\n",
    "    filename = testfile.split('.')[0]\n",
    "    img = image.load_img(TEST_PATH+'/'+testfile,target_size=(256, 256))\n",
    "    if img is None:\n",
    "        continue\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x = x /255.\n",
    "    pred = net_final.predict(x)[0]\n",
    "    tof=np.argmax(pred)\n",
    "    out = np.vstack((out,[filename, tof]))\n",
    "\n",
    "pd.DataFrame(out).to_csv(OUTPUT_PATH+'/prediction.csv',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
