{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 專案名稱: 第四屆機器學習百日馬拉松\n",
    "### 功能描述: 第45天作業\n",
    "### 版權所有: Dunk  \n",
    "### 程式撰寫: Dunk  \n",
    "### 撰寫日期：2020/04/17\n",
    "### 改版日期:  \n",
    "### 改版備註:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "你可能聽過 XGBoost/Light-GBM，這些都是資料科學競賽中最常用的機器學習模型，但其實這些演算法背後原理都是基於 Gradient-boosting 進而優化，強烈建議您對本日的課程與補充教材多花點時間閱讀與理解。 ",
    "核心概念就是透過計算梯度，來讓下一棵生成的樹能夠根據梯度方向，試圖讓 Loss 變得更小！\n",
    "\n",
    ">本日作業請完整閱讀以下任一文獻即可：\n",
    "\n",
    "- Kaggle 大師帶你了解梯度提升機原理 - 英文\n",
    "- 完整的 Ensemble 概念 by 李宏毅教授\n",
    "- 深入了解 Gradient-boosting - 英文\n",
    "\n",
    "\n",
    "完成閱讀後，請記得到下方按下完成作業。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 選擇閱讀[完整的 Ensemble 概念 by 李宏毅教授](https://www.youtube.com/watch?v=tH9FH1DH5n0 \"完整的 Ensemble 概念 by 李宏毅教授\")\n",
    ">一句話概括GBDT的核心思想就是：序列訓練n(n > 2)棵決策樹，其中第i(1 < i ≤ n)棵樹學習第i - 1棵樹的負梯度（可理解為殘差或增量），n棵樹的輸出結果累加作為最終輸出結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBM 梯度提昇機（或梯度推進機）是一種集成學習法（Ensemble）。集成學習法（也稱組合學習法）就是使用一系列學習器進行學習，並使用某種規則把各個學習結果進行整合從而獲得比單個學習器更好的學習效果的一種機器學習方法。一堆“弱學習器”組合成一個“強學習器”。正所謂“ 三個湊皮匠，合成一個諸葛亮。\n",
    "\n",
    "##### GBM 算法概念：\n",
    "\n",
    "GBM（Gradient Boosting Machine）算法是Boosting算法（提升方法）的一種。之前介紹的AdaBoost算法[請參見人工智能(40)]就是一種傳統而重要的Boost算法。Boosting（提升方法）是一大類集成學習的統稱。它用不同的權重將基學習器進行線性組合，使表現優秀的學習器得到重用。根據基學習器、損失函數和優化方法的不同，Boosting（提升方法）也有各種不同的形式。GBM算法是先根據初始模型計算偽殘差，之後建立一個基學習器來解釋偽殘差，該基學習器是在梯度方向上減少殘差。再將基學習器乘上權重係數(學習速率)和原來的模型進行線性組合形成新的模型。這樣反复迭代就可以找到一個使損失函數的期望達到最小的模型。\n",
    "\n",
    "##### GBM 算法思想：\n",
    "\n",
    "GBM 主要思想是基於之前建立的基學習器的損失函數的梯度下降方向來建立下一個新的基學習器，目的就是希望通過集成這些基學習器使得模型總體的損失函數不斷下降，模型不斷改進。\n",
    "\n",
    "##### GBM 重要參數設置：\n",
    "\n",
    "一般基學習器為決策樹，參數大致分為三類：\n",
    "\n",
    "1. 決策樹相關參數：調節模型中每個決定樹的性質，比如樹的深度、最小葉節點樣本數、最大葉節點數等；\n",
    "2. Boosting 相關參數：調節模型中boosting的操作，比如學習速率（步長）、基學習器個數、建模樣本比例等；\n",
    "3. 其他模型相關參數：調節模型總體的各項運作，比如損失函數形式、隨機種子等。\n",
    "\n",
    "下面給出GBM重要參數的設置建議：\n",
    "\n",
    "- 損失函數形式 (distribution)：損失函數的形式可以如下設定：a)分類問題一般選擇Bernoulli分佈；b)回歸問題可以選擇Gaussian分佈。\n",
    "- 迭代次數 (n)：根據經驗，迭代次數n可設置在3000-10000之間。\n",
    "- 學習速率 (shrinkage)：學習速率類似我們走路邁的步子長度，步子邁得太大容易扯著“蛋”^_^，步子邁得太小，步數增加，容易累著“腿”-迭代次數增多，訓練時間和計算資源加大。因此步子太大或太小都不合適。根據經驗，學習速率shrinkage設置在0.01-0.001之間比較合適。\n",
    "- 再抽樣比率 (fraction)：在訓練基學習器時可以使用再抽樣方法，此時就稱之為隨機梯度提升算法stochastic gradient boosting。\n",
    "- 決策樹的深度 (depth)：定義了決策樹的最大深度，它可以控製過度擬合，因為決策樹越深就越可能過度擬合。當然也應該用CV值（離散係數）檢驗。\n",
    "\n",
    "恰當地設置和調整GBM參數，GBM算法會得到意想不到的計算效果。 \n",
    "\n",
    "##### GBM 算法模型：\n",
    "\n",
    "1. 數據擬合模型：f1(x) = y\n",
    "2. 殘差擬合模型：h1(x) = y - f1(x)\n",
    "3. 構建一個新模型：f2(x) = f1(x) + h1(x)\n",
    "4. 引入更多模型來糾正前一個模型的錯誤，從而擴展了這個模型：\n",
    "\n",
    "<code>\n",
    "    \n",
    "    f（x）= f1（x）\n",
    "    f2（x）= f1（x）+ h1（x）\n",
    "    f3（x）= f2（x）+ h2（x）\n",
    "    … …\n",
    "    fm（x）= fm-1（x）+ hm-1（x）\n",
    "</code>\n",
    "\n",
    "其中，f1(x)表示擬合至y的初始模型。通過擬合f1(x)來初始化模型，之後每一步引入更過模型，目標是為了找到hm(x) = y - fm(x)。\n",
    "\n",
    "###### 注：hm(x)只是一個模型，並沒有要求hm(x)是一個基於樹形結構的模型。這正是梯度提升更寬泛的概念以及優勢之一。這只是一個通過迭代改善弱學習器效果的計算框架。在理論上，一個良好編碼的梯度提升模塊能夠根據需要隨意\"插入\"各種各樣的弱分類器。但在實踐中，hm(x)一般總是一個基於樹的學習器。\n",
    "\n",
    "##### GBM 算法流程：\n",
    "\n",
    "GBM算法思想如下：\n",
    "對於一個需要最小化的給定算法，只要能夠為其找到一個可微的損失函數，那麼就可以高效地求解。求解過程為：梯度下降+線性搜索。\n",
    "採用梯度下降的思想進行函數或參數尋優。\n",
    "\n",
    "##### GBM 算法優點：\n",
    "1. 繼承了單一決策樹的優點，又摒棄了它的缺點；\n",
    "2. 能處理缺失數據；\n",
    "3. 對於噪聲數據不敏感；\n",
    "4. 能擬合複雜的非線性關係；\n",
    "5. 精確度較高；\n",
    "6. 通過控制迭代次數能控製過度擬合；\n",
    "7. 計算速度快，性能較優。\n",
    "\n",
    "##### GBM 算法缺點：\n",
    "1. 順序計算；\n",
    "2. 不便進行分佈式計算，除非借助於一些方法；\n",
    "3. 可能會出現過擬合現象；\n",
    "4. 設置參數較多；\n",
    "5. 抗干擾能力不強。\n",
    "\n",
    "##### GBM 與RF 比較：\n",
    "1. RF隨機森林（請參加人工智能（25））通常只需要設置一個超參數即可，每個節點上隨機選取的特徵數量。一般將該參數設置為特徵總數的平方根，模型足以取得不錯的效果。而GBM梯度提昇機的超參數則包括提升樹的數量和深度、學習速率等。\n",
    "2. RF隨機森林的抗干擾性強，更不容易出現過擬合的情況。而GBM梯度提昇機的參數設置不當可能會出現過擬合的情況。\n",
    "3. 在某種意義上講，RF隨機森林是一棵比GBM梯度提昇機更加靈活的集成樹，但在一般情況下，經過良好訓練的GBM梯度提昇機性能優於隨機森林RF。\n",
    "4. RF隨機森林更容易並行化。但藉助於一些高效方法，GBM梯度提昇機同樣也能實現並行化訓練。\n",
    "\n",
    "##### GBM 算法應用：\n",
    "GBM算法可以用於回歸模型，同樣它也可以用於分類和排名模型。GBM算法現已被廣泛應用於眾多領域。\n",
    "GBM算法近年來被提及比較多的一個算法，這主要得益於其算法性能，及該算法在各類數據挖掘以及機器學習比賽中的卓越表現。\n",
    "\n",
    "##### 結語:\n",
    "GBM 梯度提昇機（或梯度推進機）是一種集成學習法（Ensemble），就是使用一系列學習器進行學習，並使用某種規則把各個學習結果進行整合從而獲得比單個學習器更好的學習效果的一種機器學習方法。GBM參數分為三類：決策樹參數、boosting參數和其他影響模型的參數。恰當地設置和調整GBM參數，GBM算法會得到意想不到的計算效果。GBM算法可以用於回歸模型，同樣它也可以用於分類和排名模型。GBM算法在眾多數據挖掘和機器學習競賽中有著卓越表現。已經被廣泛應用於眾多領域。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">GBDT\n",
    "G-B-D-T梯度提升決策樹，顧名思義，是一個與梯度有關、對決策樹進行了提升的機器學習模型。我們不妨從後往前依次聊聊GBD這幾個定語，從而理解這個模型的精髓。\n",
    ">>- DT（Decision Tree） 決策樹。 T自不必多說，作為一種常見的資料結構出現在各種演算法當中。DT決策樹，有分類樹與迴歸樹兩種，之前文章中講到了分類樹，可參見 機器學習方法篇(3)——決策樹入門 與 機器學習方法篇(4)——決策樹剪枝。迴歸樹原理機制與分類樹相似，區別在於分類樹只有在葉子結點返回唯一分類，而回歸樹的每個節點都能返回預測值，通常為當前節點下所有樣本的均值。\n",
    ">>- B（Boosting） 提升。即在原來模型的基礎之上做進一步提升，提升決策樹BDT的基本思想是採用多棵決策樹序列建模。具體過程為，對於第一棵樹之後的每一棵決策樹，都基於前一棵決策樹的輸出進行二次建模，整個序列建模過程相當於對預測結果朝目標值進行修正。\n",
    ">>- G（Gradient） 梯度。梯度的大小反映了當前預測值與目標值之間的距離。因此，上面B所述的序列決策樹模型，除開第一棵決策樹使用原始預測指標建樹，之後的每一棵決策樹都用前一棵決策樹的預測值與目標值計算出來的負梯度（可以理解為殘差或者增量）來建樹。這相當於給分錯的樣本加權多次分類，使樣本最終的殘差趨近於0。除開第一棵樹的其他樹，由於都是對目標的殘差或增量進行建模預測，因此GBDT模型只需把過程中每一棵決策樹的輸出結果累加，便可得到最終的預測輸出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 參考資料\n",
    "[機器學習馬拉松 045 Tree Based Model 梯度提升機介紹](https://medium.com/uxai/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E9%A6%AC%E6%8B%89%E6%9D%BE-045-tree-based-model-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A9%9F%E4%BB%8B%E7%B4%B9-a84d3059e13 \"機器學習馬拉松 045 Tree Based Model 梯度提升機介紹\")\n",
    "\n",
    "[機器學習方法篇(9)------梯度提升決策樹GBDT](https://www.itread01.com/content/1546105709.html \"機器學習方法篇(9)------梯度提升決策樹GBDT\")\n",
    "\n",
    "[人工智能之GBM算法](https://mp.ofweek.com/ai/a645673021216 \"人工智能之GBM算法\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
