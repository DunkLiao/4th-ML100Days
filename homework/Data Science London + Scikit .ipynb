{"cells":[{"metadata":{"trusted":true,"_uuid":"c971dea1c053cbc4c5f3cc037258c5db4ef61a64","_cell_guid":"81ccc097-f770-4521-93d1-3ce0dcda7c40"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"febb07bf507ba6fa41aa45dfce5d37500b122128","_cell_guid":"a032708c-a289-4ff9-a221-9356af1d40e3"},"cell_type":"code","source":"# read csv (comma separated value) into data\ntrain = pd.read_csv('../input/train.csv', header=None)\ntrainLabel = pd.read_csv('../input/trainLabels.csv', header=None)\ntest = pd.read_csv('../input/test.csv', header=None)\nprint(plt.style.available) # look at available plot styles\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b0433d31468004f3b60a71edba61d036c03d665","_cell_guid":"87f421e2-a229-4dcd-8be5-317fd40d576c"},"cell_type":"code","source":"print('train shape:', train.shape)\nprint('test shape:', test.shape)\nprint('trainLabel shape:', trainLabel.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc0d8923493c5a705fb693fead1d29a8dc9dc12f","_cell_guid":"d77d8c72-e884-465e-9b31-c2f5c0d731dd"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"497dea6e73fe8af73eac080452de05ea9e9b54f5","_cell_guid":"f7dd7d49-94d7-44ec-b59b-1dada4ebe44b"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3902eda769a55de4332792a643c2ef375b66e478","_cell_guid":"d861ce8f-ce78-4d98-a47a-27b1a7a3b6ae"},"cell_type":"markdown","source":"# **Use only kNN for classification**"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9497beea005c7c0ff511b89896d6333ce6b45a2e","_cell_guid":"4681c0c6-e61b-40e4-9999-6c4386b37024"},"cell_type":"code","source":"# KNN with cross-validation\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nX, y = train, np.ravel(trainLabel)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a90c4f472700240c58501ebdafd4ffe4640d39e2","_cell_guid":"bc78572a-6c14-4a18-84e0-78629b31a514"},"cell_type":"code","source":"# Model complexity\nneig = np.arange(1, 25)\nkfold = 10\ntrain_accuracy = []\nval_accuracy = []\nbestKnn = None\nbestAcc = 0.0\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(X_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(X_train, y_train))\n    # test accuracy\n    val_accuracy.append(np.mean(cross_val_score(knn, X, y, cv=kfold)))\n    if np.mean(cross_val_score(knn, X, y, cv=kfold)) > bestAcc:\n        bestAcc = np.mean(cross_val_score(knn, X, y, cv=10))\n        bestKnn = knn\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, val_accuracy, label = 'Validation Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('k value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.show()\n\nprint('Best Accuracy without feature scaling:', bestAcc)\nprint(bestKnn)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4924f19a395fa0c15c6741495a4b28f668e169d4","_cell_guid":"96ab61ed-de0e-45b0-b606-8f73ebb75e75","scrolled":false},"cell_type":"code","source":"# predict test\ntest_fill = np.nan_to_num(test)\nsubmission = pd.DataFrame(bestKnn.predict(test_fill))\nprint(submission.shape)\nsubmission.columns = ['Solution']\nsubmission['Id'] = np.arange(1,submission.shape[0]+1)\nsubmission = submission[['Id', 'Solution']]\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"28b57a3f2f6c912d61a3cf7644930f25dfdd9221","_cell_guid":"b51fd426-5e72-4ac3-942b-554fc79b4e48"},"cell_type":"code","source":"submission.to_csv('submission_no_normalization.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"edd0c76f693269e71862a15e1b57f2bf2b86a87a","_cell_guid":"71b2ea90-35fb-4cea-9fa4-bc6fcffbdc61"},"cell_type":"code","source":"print(check_output([\"ls\", \"../working\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"ffa9d0ac2ea92ee1708c506bcd35cd5ef552a2bf","_cell_guid":"10999629-c16f-4960-ba6b-c8222b0a4ed6"},"cell_type":"markdown","source":"# **Add feature scaling**"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8ada15ed0f81aafb06b1a62def3aecc2f38f2a79","_cell_guid":"9a827a80-d532-4b8a-973b-7c0a1aeeb372"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n\nstd = StandardScaler()\nX_std = std.fit_transform(X)\nmms = MinMaxScaler()\nX_mms = mms.fit_transform(X)\nnorm = Normalizer()\nX_norm = norm.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f4e2465bc8355a4dad17db2d73f43978b21f5adb","_cell_guid":"7d7b6c3c-5be1-4be8-a181-446f738e1386"},"cell_type":"code","source":"# Model complexity\nneig = np.arange(1, 30)\nkfold = 10\nval_accuracy = {'std':[], 'mms':[], 'norm':[]}\nbestKnn = None\nbestAcc = 0.0\nbestScaling = None\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # validation accuracy\n    s1 = np.mean(cross_val_score(knn, X_std, y, cv=kfold))\n    val_accuracy['std'].append(s1)\n    s2 = np.mean(cross_val_score(knn, X_mms, y, cv=kfold))\n    val_accuracy['mms'].append(s2)\n    s3 = np.mean(cross_val_score(knn, X_norm, y, cv=kfold))\n    val_accuracy['norm'].append(s3)\n    if s1 > bestAcc:\n        bestAcc = s1\n        bestKnn = knn\n        bestScaling = 'std'\n    elif s2 > bestAcc:\n        bestAcc = s2\n        bestKnn = knn\n        bestScaling = 'mms'\n    elif s3 > bestAcc:\n        bestAcc = s3\n        bestKnn = knn\n        bestScaling = 'norm'\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, val_accuracy['std'], label = 'CV Accuracy with std')\nplt.plot(neig, val_accuracy['mms'], label = 'CV Accuracy with mms')\nplt.plot(neig, val_accuracy['norm'], label = 'CV Accuracy with norm')\nplt.legend()\nplt.title('k value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.show()\n\nprint('Best Accuracy with feature scaling:', bestAcc)\nprint('Best kNN classifier:', bestKnn)\nprint('Best scaling:', bestScaling)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c281d9400c0fdfd5fbb58dde96d8cdb6918ee160","_cell_guid":"ec5f98d4-a67e-4a6e-a3e7-b70833f5ba61"},"cell_type":"code","source":"# predict on test\nbestKnn.fit(X_norm, y)\nsubmission = pd.DataFrame(bestKnn.predict(norm.transform(test_fill)))\nprint(submission.shape)\nsubmission.columns = ['Solution']\nsubmission['Id'] = np.arange(1,submission.shape[0]+1)\nsubmission = submission[['Id', 'Solution']]\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4f4b8c93a07cd86d43c644e2a90b34fcb1ebe171","_cell_guid":"71844f66-639e-40b7-97e9-ff2da30cf1b7"},"cell_type":"code","source":"submission.to_csv('submission_with_scaling.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"99f33cdb92ad33e15568dfb81a78caaf0bdf49c5","_cell_guid":"3c6923a9-e8a5-4035-992e-58cad694e482"},"cell_type":"code","source":"print(check_output([\"ls\", \"../working\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"685ef1dc7fd475848046296983ca807008b5841a","_cell_guid":"c87995ab-8cdb-431c-b7a7-e798daffc2db"},"cell_type":"markdown","source":"# **Feature Selection**"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9783c64eb1641f03a55e535381d6fe02ee5a122f","_cell_guid":"2f48eb29-eb08-4513-aabd-04f6a7464cc0"},"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(pd.DataFrame(X_std).corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"346af7b1575fbdb931cc64442451bcbdabad4a17","_cell_guid":"21a73a07-c2c0-4c1d-9506-3ebb0aa95969"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and val 30 %\nX_train, X_val, y_train, y_val = train_test_split(X_std, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(X_train,y_train)\n\nac = accuracy_score(y_val,clf_rf.predict(X_val))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_val,clf_rf.predict(X_val))\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e5553d8f1708442f079bcd800c86a97ff810243","_cell_guid":"bed9d8f7-eab0-4e25-8d76-4287397e1b8b"},"cell_type":"markdown","source":"Univariate feature selection is not very accurate because independent variables may correlate with each other.\nBut it can give us a concept. Here we directly use more robust RFECV."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"62a79b1b0b00928238c76b333d177bf7fe1c9c7a","_cell_guid":"a6df7a5b-c8ff-4e01-bcdb-e7998dbaf1c4"},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.feature_selection import RFECV\n\nkfold = 10\nbestSVC = None\nbestAcc = 0.0\nval_accuracy = []\ncv_range = np.arange(5, 11)\nn_feature = []\nfor cv in cv_range:\n    # Create the RFE object and compute a cross-validated score.\n    svc = SVC(kernel=\"linear\")\n    # The \"accuracy\" scoring is proportional to the number of correct\n    # classifications\n    rfecv = RFECV(estimator=svc, step=1, cv=cv, scoring='accuracy')\n    rfecv.fit(X_std, y)\n\n    # print(\"Optimal number of features : %d\" % rfecv.n_features_)\n    # print('Best features :', pd.DataFrame(X_train).columns[rfecv.support_])\n\n    # Model complexity\n    val_accuracy += [np.mean(cross_val_score(svc, X_std[:, rfecv.support_], y, cv=kfold))]\n    n_feature.append(rfecv.n_features_)\n    if val_accuracy[-1] > bestAcc:\n        bestAcc = val_accuracy[-1]\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(cv_range, val_accuracy, label = 'CV Accuracy')\nfor i in range(len(cv_range)):\n    plt.annotate(str(n_feature[i]), xy=(cv_range[i],val_accuracy[i]))\nplt.legend()\nplt.title('Cross Validation Accuracy')\nplt.xlabel('k fold')\nplt.ylabel('Accuracy')\nplt.show()\n\nprint('Best Accuracy with feature scaling and RFECV:', bestAcc)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"54605e40c93f2fff1b0e00b21ed3e462c078b616","_cell_guid":"adb65715-79c7-46df-8ad0-63f087f0fa08"},"cell_type":"code","source":"import numpy as np\n#import sklearn as sk\n#import matplotlib.pyplot as plt\nimport pandas as pd\n\n#from sklearn.linear_model import LogisticRegression\n#from sklearn.linear_model import Perceptron\n#from sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import VotingClassifier\n#from sklearn import svm\n\n#### READING OUR GIVEN DATA INTO PANDAS DATAFRAME ####\nx_train = train\ny_train = trainLabel\nx_test = test\nx_train = np.asarray(x_train)\ny_train = np.asarray(y_train)\nx_test = np.asarray(x_test)\ny_train = y_train.ravel()\nprint('training_x Shape:',x_train.shape,',training_y Shape:',y_train.shape, ',testing_x Shape:',x_test.shape)\n\n#Checking the models\nx_all = np.r_[x_train,x_test]\nprint('x_all shape :',x_all.shape)\n\n#### USING THE GAUSSIAN MIXTURE MODEL ####\nfrom sklearn.mixture import GaussianMixture\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        # Fit a mixture of Gaussians with EM\n        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n        gmm.fit(x_all)\n        bic.append(gmm.aic(x_all))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n            \nbest_gmm.fit(x_all)\nx_train = best_gmm.predict_proba(x_train)\nx_test = best_gmm.predict_proba(x_test)\n\n\n#### TAKING ONLY TWO MODELS FOR KEEPING IT SIMPLE ####\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier()\n\nparam_grid = dict( )\n#### GRID SEARCH for BEST TUNING PARAMETERS FOR KNN #####\ngrid_search_knn = GridSearchCV(knn,param_grid=param_grid,cv=10,scoring='accuracy').fit(x_train,y_train)\nprint('best estimator KNN:',grid_search_knn.best_estimator_,'Best Score', grid_search_knn.best_estimator_.score(x_train,y_train))\nknn_best = grid_search_knn.best_estimator_\n\n#### GRID SEARCH for BEST TUNING PARAMETERS FOR RandomForest #####\ngrid_search_rf = GridSearchCV(rf, param_grid=dict( ), verbose=3,scoring='accuracy',cv=10).fit(x_train,y_train)\nprint('best estimator RandomForest:',grid_search_rf.best_estimator_,'Best Score', grid_search_rf.best_estimator_.score(x_train,y_train))\nrf_best = grid_search_rf.best_estimator_\n\n\nknn_best.fit(x_train,y_train)\nprint(knn_best.predict(x_test)[0:10])\nrf_best.fit(x_train,y_train)\nprint(rf_best.predict(x_test)[0:10])\n\n#### SCORING THE MODELS ####\nprint('Score for KNN :',cross_val_score(knn_best,x_train,y_train,cv=10,scoring='accuracy').mean())\nprint('Score for Random Forest :',cross_val_score(rf_best,x_train,y_train,cv=10,scoring='accuracy').max())\n\n### IN CASE WE WERE USING MORE THAN ONE CLASSIFIERS THEN VOTING CLASSIFIER CAN BE USEFUL ###\n#clf = VotingClassifier(\n#\t\testimators=[('knn_best',knn_best),('rf_best',rf_best)],\n#\t\t#weights=[871856020222,0.907895269918]\n#\t)\n#clf.fit(x_train,y_train)\n#print clf.predict(x_test)[0:10]\n\n##### FRAMING OUR SOLUTION #####\nknn_best_pred = pd.DataFrame(knn_best.predict(x_test))\nrf_best_pred = pd.DataFrame(rf_best.predict(x_test))\n#voting_clf_pred = pd.DataFrame(clf.predict(x_test))\n\nknn_best_pred.index += 1\nrf_best_pred.index += 1\n#voting_clf_pred.index += 1\n\nrf_best_pred.columns = ['Solution']\nrf_best_pred['Id'] = np.arange(1,rf_best_pred.shape[0]+1)\nrf_best_pred = rf_best_pred[['Id', 'Solution']]\nprint(rf_best_pred)\n\n#knn_best_pred.to_csv('knn_best_pred.csv')\nrf_best_pred.to_csv('Submission_rf.csv', index=False)\n#voting_clf_pred.to_csv('voting_clf_pred.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"483ad23b61e05402df568c73dfdccf37e6101660","_cell_guid":"e1d06620-7d84-4c7b-a043-bb40dd7711ac"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}